{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and Credits\n",
    "This RNN model is based on Mat Leonard's Sentiment Prediction course on Udacity and the course materials he provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnntext.txt', 'r') as f: #inputs\n",
    "    texts = f.read()\n",
    "with open('rnntype.txt', 'r') as f: #labels\n",
    "    types = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replace every \"What if...\" with \"Why the fuck not\\nThere are two ways to reach me: by way of kisses or by way of the imagination. But there is a hierarchy: the kisses alone don\\'t work.\\nScience is not separate from politics. As much as I would like it to be a pure thing, existing only in some intellectual realm unsullied by human struggle, it will always be entangled with the world we live in.\\nThink those fake spider webs on your bushes are spooky? London woke up to an extra creepy layer of fog Monday morning, just in time for Halloween.\\nIn difficult times, the only strategy which works is -\\'Patience.\\nThere is no secret ingredient\\nThere is always something Right in something that is Wrong and something Wrong in something that is Right.\\nDrone footage shows Rohingya refugees fleeing in a mass exodus from Myanmar into Bangladesh\\nThere is no religion without love. Accept who you are and revel in it\\nWar is an ugly thing, but not the ugliest of things: the decayed and degraded state of moral a'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "all_text = ''.join([c.lower() for c in texts if c not in punctuation]) #lowercasing, punctuations removed\n",
    "texts = all_text.split('\\n')\n",
    "\n",
    "#Tokenizing\n",
    "quotes = []\n",
    "for text in texts:\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    quotes.append(' '.join(tokens))    \n",
    "texts = quotes\n",
    "\n",
    "#Optional: Stemming\n",
    "'''\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "texts_stem = []\n",
    "for text in texts:\n",
    "    quote_stem = []\n",
    "    for token in text.split(' '):\n",
    "        stem = snowball.stem(token)\n",
    "        quote_stem.append(stem)\n",
    "    texts_stem.append(' '.join(quote_stem))    \n",
    "texts = texts_stem\n",
    "'''\n",
    "#Optional: Lemmatizing\n",
    "'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "texts_lemma = []\n",
    "for text in texts:\n",
    "    quote_lemma = []\n",
    "    for word, tag in pos_tag(word_tokenize(text)):\n",
    "        wntag = tag[0].lower()\n",
    "        if wntag == 'j':\n",
    "            wntag = 'a'\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        quote_lemma.append(lemma)\n",
    "    texts_lemma.append(' '.join(quote_lemma))    \n",
    "texts = texts_lemma\n",
    "'''\n",
    "all_text = ' '.join(texts)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'replace every what if with why the fuck not there be two way to reach me by way of kiss or by way of the imagination but there be a hierarchy the kiss'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replace',\n",
       " 'every',\n",
       " 'what',\n",
       " 'if',\n",
       " 'with',\n",
       " 'why',\n",
       " 'the',\n",
       " 'fuck',\n",
       " 'not',\n",
       " 'there',\n",
       " 'be',\n",
       " 'two',\n",
       " 'way',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'me',\n",
       " 'by',\n",
       " 'way',\n",
       " 'of',\n",
       " 'kiss']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True) #descending\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} \n",
    "\n",
    "texts_ints = []\n",
    "for text in texts:\n",
    "    texts_ints.append([vocab_to_int[word] for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtypes = str(types)\n",
    "types = strtypes.split('\\n')\n",
    "types = np.array([1 if type == 'mundane' else 0 for type in types])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length texts: 0\n",
      "Maximum text length: 628\n"
     ]
    }
   ],
   "source": [
    "text_lens = Counter([len(x) for x in texts_ints])\n",
    "print(\"Zero-length texts: {}\".format(text_lens[0]))\n",
    "print(\"Maximum text length: {}\".format(max(text_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx = [ii for ii, text in enumerate(texts_ints) if len(text) != 0]\n",
    "len(non_zero_idx) #check if there is any empty input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ints = [texts_ints[ii] for ii in non_zero_idx]\n",
    "types = np.array([types[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = np.zeros((len(texts_ints), seq_len), dtype=int) #feft padding with 0's if an input has fewer than 200 words\n",
    "for i, row in enumerate(texts_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 2138,    3,   50,  258,    9,  286,    1, 1216,   99,  124,\n",
       "          2, 1880, 2275,    4,   26,  135, 1053,    4,  383,    9,  286,\n",
       "          9,   78,    1,  130,    3,  177,  330,    5,   46, 6622,    5,\n",
       "          1,  421,    3,  710, 2468,   29,  396,   20,    6, 4429, 1412,\n",
       "          4,    2, 1321,   20,   77, 1840])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[92,150:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(14800, 200) \n",
      "Validation set: \t(1850, 200) \n",
      "Test set: \t\t(1850, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[3700:], features[:3700]\n",
    "train_y, val_y = types[3700:], types[:3700]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the hyperparameters.\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    texts_ = tf.placeholder(tf.int32, [None, None], name='texts')\n",
    "    types_ = tf.placeholder(tf.int32, [None, None], name='types')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, texts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(types_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), types_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.202\n",
      "Val acc: 0.737\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.135\n",
      "Val acc: 0.791\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.105\n",
      "Val acc: 0.845\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.124\n",
      "Val acc: 0.826\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.124\n",
      "Val acc: 0.865\n",
      "Epoch: 1/10 Iteration: 30 Train loss: 0.086\n",
      "Val acc: 0.868\n",
      "Epoch: 1/10 Iteration: 35 Train loss: 0.060\n",
      "Val acc: 0.872\n",
      "Epoch: 1/10 Iteration: 40 Train loss: 0.069\n",
      "Val acc: 0.881\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.068\n",
      "Val acc: 0.882\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.044\n",
      "Val acc: 0.885\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.043\n",
      "Val acc: 0.887\n",
      "Epoch: 2/10 Iteration: 60 Train loss: 0.040\n",
      "Val acc: 0.893\n",
      "Epoch: 2/10 Iteration: 65 Train loss: 0.038\n",
      "Val acc: 0.889\n",
      "Epoch: 2/10 Iteration: 70 Train loss: 0.040\n",
      "Val acc: 0.881\n",
      "Epoch: 2/10 Iteration: 75 Train loss: 0.039\n",
      "Val acc: 0.889\n",
      "Epoch: 2/10 Iteration: 80 Train loss: 0.027\n",
      "Val acc: 0.892\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.040\n",
      "Val acc: 0.889\n",
      "Epoch: 3/10 Iteration: 90 Train loss: 0.025\n",
      "Val acc: 0.884\n",
      "Epoch: 3/10 Iteration: 95 Train loss: 0.024\n",
      "Val acc: 0.885\n",
      "Epoch: 3/10 Iteration: 100 Train loss: 0.021\n",
      "Val acc: 0.889\n",
      "Epoch: 3/10 Iteration: 105 Train loss: 0.020\n",
      "Val acc: 0.891\n",
      "Epoch: 3/10 Iteration: 110 Train loss: 0.016\n",
      "Val acc: 0.891\n",
      "Epoch: 3/10 Iteration: 115 Train loss: 0.037\n",
      "Val acc: 0.895\n",
      "Epoch: 4/10 Iteration: 120 Train loss: 0.009\n",
      "Val acc: 0.887\n",
      "Epoch: 4/10 Iteration: 125 Train loss: 0.009\n",
      "Val acc: 0.883\n",
      "Epoch: 4/10 Iteration: 130 Train loss: 0.016\n",
      "Val acc: 0.887\n",
      "Epoch: 4/10 Iteration: 135 Train loss: 0.022\n",
      "Val acc: 0.886\n",
      "Epoch: 4/10 Iteration: 140 Train loss: 0.024\n",
      "Val acc: 0.887\n",
      "Epoch: 4/10 Iteration: 145 Train loss: 0.014\n",
      "Val acc: 0.887\n",
      "Epoch: 5/10 Iteration: 150 Train loss: 0.008\n",
      "Val acc: 0.879\n",
      "Epoch: 5/10 Iteration: 155 Train loss: 0.023\n",
      "Val acc: 0.883\n",
      "Epoch: 5/10 Iteration: 160 Train loss: 0.014\n",
      "Val acc: 0.867\n",
      "Epoch: 5/10 Iteration: 165 Train loss: 0.005\n",
      "Val acc: 0.882\n",
      "Epoch: 5/10 Iteration: 170 Train loss: 0.024\n",
      "Val acc: 0.886\n",
      "Epoch: 6/10 Iteration: 175 Train loss: 0.012\n",
      "Val acc: 0.879\n",
      "Epoch: 6/10 Iteration: 180 Train loss: 0.011\n",
      "Val acc: 0.880\n",
      "Epoch: 6/10 Iteration: 185 Train loss: 0.013\n",
      "Val acc: 0.876\n",
      "Epoch: 6/10 Iteration: 190 Train loss: 0.011\n",
      "Val acc: 0.864\n",
      "Epoch: 6/10 Iteration: 195 Train loss: 0.017\n",
      "Val acc: 0.885\n",
      "Epoch: 6/10 Iteration: 200 Train loss: 0.011\n",
      "Val acc: 0.879\n",
      "Epoch: 7/10 Iteration: 205 Train loss: 0.007\n",
      "Val acc: 0.883\n",
      "Epoch: 7/10 Iteration: 210 Train loss: 0.020\n",
      "Val acc: 0.876\n",
      "Epoch: 7/10 Iteration: 215 Train loss: 0.007\n",
      "Val acc: 0.877\n",
      "Epoch: 7/10 Iteration: 220 Train loss: 0.010\n",
      "Val acc: 0.882\n",
      "Epoch: 7/10 Iteration: 225 Train loss: 0.013\n",
      "Val acc: 0.867\n",
      "Epoch: 7/10 Iteration: 230 Train loss: 0.017\n",
      "Val acc: 0.889\n",
      "Epoch: 8/10 Iteration: 235 Train loss: 0.018\n",
      "Val acc: 0.885\n",
      "Epoch: 8/10 Iteration: 240 Train loss: 0.006\n",
      "Val acc: 0.867\n",
      "Epoch: 8/10 Iteration: 245 Train loss: 0.012\n",
      "Val acc: 0.870\n",
      "Epoch: 8/10 Iteration: 250 Train loss: 0.010\n",
      "Val acc: 0.884\n",
      "Epoch: 8/10 Iteration: 255 Train loss: 0.006\n",
      "Val acc: 0.885\n",
      "Epoch: 8/10 Iteration: 260 Train loss: 0.014\n",
      "Val acc: 0.884\n",
      "Epoch: 9/10 Iteration: 265 Train loss: 0.005\n",
      "Val acc: 0.886\n",
      "Epoch: 9/10 Iteration: 270 Train loss: 0.004\n",
      "Val acc: 0.887\n",
      "Epoch: 9/10 Iteration: 275 Train loss: 0.008\n",
      "Val acc: 0.880\n",
      "Epoch: 9/10 Iteration: 280 Train loss: 0.009\n",
      "Val acc: 0.867\n",
      "Epoch: 9/10 Iteration: 285 Train loss: 0.008\n",
      "Val acc: 0.865\n",
      "Epoch: 9/10 Iteration: 290 Train loss: 0.007\n",
      "Val acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "### Make sure the checkpoints directory exists\n",
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {texts_: x,\n",
    "                    types_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%5==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {texts_: x,\n",
    "                            types_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test accuracy: 0.892\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {texts_: x,\n",
    "                types_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
