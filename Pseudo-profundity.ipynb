{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def load_and_shuffle(text_file, label_file):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(text_file, 'r') as f:\n",
    "        for text in f:\n",
    "            tokenized = ' '.join([token.lower() for token in word_tokenize(text) if token.isalpha()])\n",
    "            texts.append(tokenized.strip())\n",
    "    with open(label_file, 'r') as f:\n",
    "        for label in f:\n",
    "            if 'vacuous' in label:\n",
    "                labels.append(1) \n",
    "            else:\n",
    "                labels.append(0) \n",
    "              \n",
    "    combined = list(zip(texts, labels)) #shuffling\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    return zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_and_shuffle('quotes.txt', 'labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(sent):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    sentence = []\n",
    "    for word, tag in pos_tag(word_tokenize(sent)):\n",
    "        wntag = tag[0].lower()\n",
    "        if wntag == 'j': #rename adjective tags\n",
    "            wntag = 'a'\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None #adjectives, adverbs, nouns, verbs\n",
    "        if not wntag:\n",
    "            lemma = word #unlemmatized\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        sentence.append(lemma)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lemma = []\n",
    "for text in texts:\n",
    "    texts_lemma.append(lemmatize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(texts, labels, k):\n",
    "    length = len(texts)//k\n",
    "    folds = []\n",
    "    folds.append([texts, labels])\n",
    "    for i in range(1, k):\n",
    "        inputs = texts[i*length:] + texts[:i*length]\n",
    "        outputs = labels[i*length:] + labels[:i*length]\n",
    "        folds.append([inputs, outputs])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = k_folds(texts, labels, 5)\n",
    "folds_lemma = k_folds(texts_lemma, labels, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 8\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = int(len(labels) * 0.8)\n",
    "num_epochs = 30\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "adam_for_RNN = tf.keras.optimizers.Adam(learning_rate=0.0001) #for simpleRNN,LSTM, biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencing_and_padding(texts, labels):\n",
    "    training_texts = texts[0:training_size]\n",
    "    testing_texts = texts[training_size:]\n",
    "    training_labels = labels[0:training_size]\n",
    "    testing_labels = labels[training_size:]\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_texts)\n",
    "    ### Save the tokenizer ###\n",
    "    '''\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    '''\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_texts)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_texts)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    return [np.array(training_padded), np.array(training_labels), np.array(testing_padded), np.array(testing_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_padded = []\n",
    "for fold in folds:\n",
    "    folds_padded.append(sequencing_and_padding(fold[0], fold[1]))\n",
    "folds_lemma_padded = []\n",
    "for fold in folds_lemma:\n",
    "    folds_lemma_padded.append(sequencing_and_padding(fold[0], fold[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = tf.keras.Sequential([ #list of layers to add to the model\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), #10k * 8 = 80k\n",
    "    tf.keras.layers.GlobalAveragePooling1D(), #if Flatten() crashes\n",
    "    tf.keras.layers.Dense(8, activation='tanh'), #(8+1)*8\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') #(8+1)\n",
    "])\n",
    "    \n",
    "model_MLP.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Precision'])\n",
    "model_MLP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_MLP.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    MLP_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_MLP.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    MLP_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(32, 5, activation='relu'), # [i × f × o] + o, (i = 8, f = 2, o = 4)\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(8, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Precision'])\n",
    "model_CNN.summary()\n",
    "### Save the model ###\n",
    "#model_CNN.save('1DConv_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_CNN.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    CNN_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_CNN.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    CNN_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simpleRNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.SimpleRNN(32),\n",
    "    tf.keras.layers.Dense(8, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_simpleRNN.compile(loss = 'binary_crossentropy', optimizer = adam_for_RNN, metrics=['Precision'])\n",
    "model_simpleRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleRNN_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_simpleRNN.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    simpleRNN_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleRNN_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_simpleRNN.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    simpleRNN_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GRU(32),\n",
    "    tf.keras.layers.Dense(8, activation='tanh'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_GRU.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['Precision'])\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_GRU.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    GRU_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_GRU.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    GRU_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(32), \n",
    "    tf.keras.layers.Dense(8, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid') \n",
    "]) \n",
    "\n",
    "model_LSTM.compile(loss='binary_crossentropy', optimizer = adam_for_RNN, metrics=['Precision'])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LSTM_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_LSTM.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    LSTM_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_LSTM.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    LSTM_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_biLSTM = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(8, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model_biLSTM.compile(loss='binary_crossentropy', optimizer = adam_for_RNN, metrics=['Precision'])\n",
    "model_biLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biLSTM_history = []\n",
    "for fold_padded in folds_padded:\n",
    "    history = model_biLSTM.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    biLSTM_history.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biLSTM_history_lemma = []\n",
    "for fold_padded in folds_lemma_padded:\n",
    "    history = model_biLSTM.fit(fold_padded[0], fold_padded[1], epochs = num_epochs, \n",
    "                            validation_data = (fold_padded[2], fold_padded[3]), callbacks=[callback], verbose = 1)\n",
    "    biLSTM_history_lemma.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "def cross_validating(histories):\n",
    "    prec = [0]*num_epochs\n",
    "    val_prec = [0]*num_epochs\n",
    "    for history in histories:\n",
    "        prec = list(map(add, prec, history.history['Precision']))\n",
    "        val_prec = list(map(add, val_prec, history.history['val_Precision']))\n",
    "    return [precision/5 for precision in prec], [val_precision/5 for val_precision in val_prec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_prec, MLP_val_prec = cross_validating(MLP_history)\n",
    "MLP_lemma_prec, MLP_lemma_val_prec = cross_validating(MLP_history_lemma)\n",
    "CNN_prec, CNN_val_prec = cross_validating(CNN_history)\n",
    "CNN_lemma_prec, CNN_lemma_val_prec = cross_validating(CNN_history_lemma)\n",
    "simpleRNN_prec, simpleRNN_val_prec = cross_validating(simpleRNN_history)\n",
    "simpleRNN_lemma_prec, simpleRNN_lemma_val_prec = cross_validating(simpleRNN_history_lemma)\n",
    "GRU_prec, GRU_val_prec = cross_validating(GRU_history)\n",
    "GRU_lemma_prec, GRU_lemma_val_prec = cross_validating(GRU_history_lemma)\n",
    "LSTM_prec, LSTM_val_prec = cross_validating(LSTM_history)\n",
    "LSTM_lemma_prec, LSTM_lemma_val_prec = cross_validating(LSTM_history_lemma)\n",
    "biLSTM_prec, biLSTM_val_prec = cross_validating(biLSTM_history)\n",
    "biLSTM_lemma_prec, biLSTM_lemma_val_prec = cross_validating(biLSTM_history_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleRNN_variance = np.array(simpleRNN_prec) - np.array(simpleRNN_val_prec)\n",
    "simpleRNN_lemma_variance = np.array(simpleRNN_lemma_prec) - np.array(simpleRNN_lemma_val_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(simpleRNN_variance, simpleRNN_lemma_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MLP_prec, '--o', MLP_val_prec, '--o', MLP_lemma_prec, '--o', MLP_lemma_val_prec, '--o', markersize=3.5, linewidth=1.5)\n",
    "#plt.gca().fill_between(epoch, MLP_prec, MLP_lemma_prec, facecolor = 'grey', alpha = 0.5)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision') \n",
    "plt.title('MLP')\n",
    "plt.legend(['Precision', 'Validation Precision', 'Precision (lemma)', 'Validation Precision (lemma)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MLP_val_prec, '--o', CNN_val_prec, '--o', simpleRNN_val_prec, '--o', GRU_val_prec, '--o',\n",
    "         LSTM_val_prec, '--o', biLSTM_val_prec, '--o', markersize=3.5, linewidth=1.5)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision') \n",
    "plt.title('Models')\n",
    "plt.legend(['MLP', 'CNN', 'SimpleRNN', 'GRU', 'LSTM', 'BiLSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MLP_lemma_val_prec, '--o', CNN_lemma_val_prec, '--o', simpleRNN_lemma_val_prec, '--o', GRU_lemma_val_prec, '--o',\n",
    "         LSTM_lemma_val_prec, '--o', biLSTM_lemma_val_prec, '--o', markersize=3.5, linewidth=1.5)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision') \n",
    "plt.title('Models (lemma)')\n",
    "plt.legend(['MLP', 'CNN', 'SimpleRNN', 'GRU', 'LSTM', 'BiLSTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild an MLP for Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(texts)\n",
    "corpus = corpus.split()\n",
    "word_counts = {}\n",
    "for token in corpus:\n",
    "    if word_counts.get(token):\n",
    "        word_counts[token] = word_counts.get(token) + 1\n",
    "    else:\n",
    "        word_counts[token] = 1\n",
    "new_texts = []\n",
    "for text in texts:\n",
    "    cleaned = ''\n",
    "    for token in text.split():\n",
    "        if word_counts[token] > 10:\n",
    "            cleaned += token + ' '\n",
    "    new_texts.append(cleaned.strip())\n",
    "size = len(set(' '.join(new_texts).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP_full = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Embedding(size, 8, input_length=150), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(8, activation='tanh'), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "    \n",
    "model_MLP_full.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_MLP_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(new_texts)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(texts)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=150, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_full = model_MLP_full.fit(training_padded, np.array(labels), epochs = 15, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acqure weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
    "\n",
    "e = model_MLP_full.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy.linalg import norm\n",
    "def most_similar(word):\n",
    "    reversed_index = word_index.get(word, 0) #1-indexed\n",
    "    if reversed_index > 5001 or reversed_index == 0:\n",
    "        return '<OOV>'\n",
    "    similarities = Counter()\n",
    "    for i in range(1,5001):\n",
    "        weight_self = weights[reversed_index-1]\n",
    "        weight = weights[i-1]\n",
    "        similarities[reverse_word_index[i]] = round(np.dot(weight_self, weight) / (norm(weight_self) * norm(weight)),5)\n",
    "    return similarities.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('flaws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output weights\n",
    "metadat and vectors can be uploaded at http://projector.tensorflow.org/ for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs_pseudo-profound.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta_pseudo-profound.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, 5001):\n",
    "  word = reverse_word_index[word_num] #1-indexed\n",
    "  embeddings = weights[word_num-1] #0-indexed\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = [\"You are basic!\", 'Live, laugh, love!']\n",
    "sequences = tokenizer.texts_to_sequences(example_texts )\n",
    "padded = pad_sequences(sequences, maxlen=150, padding=padding_type, truncating=trunc_type)\n",
    "print(model_MLP_full.predict(padded)[0][0])\n",
    "print(model_MLP_full.predict(padded)[1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
